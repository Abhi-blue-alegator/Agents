{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langgraph langchain-openai langchain-community python-docx python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import datetime\n",
    "from typing import TypedDict, List, Literal, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    conversation_history: Annotated[List[str], operator.add]\n",
    "    test_report: str\n",
    "    generated_questions: List[str]\n",
    "    pending_questions: List[str]\n",
    "    last_follow_up: datetime.datetime\n",
    "    symptoms_collected: bool\n",
    "    report_processed: bool\n",
    "    next_action: Literal[\"collect_symptoms\", \"process_report\", \"clarify_questions\", \"follow_up\", \"await_input\", \"exit\"]\n",
    "    user_input: str\n",
    "\n",
    "# Initialize LLMs\n",
    "llm = ChatOpenAI(temperature=0.2, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_node(state: AgentState):\n",
    "    \"\"\" Supervisor decides next action based on user input \"\"\"\n",
    "    try:\n",
    "        # If no input received, wait\n",
    "        if not state.get(\"user_input\", \"\").strip():\n",
    "            print(\"[Supervisor] Awaiting patient input...\")\n",
    "            return {\"next_action\": \"await_input\"}  # NEW state\n",
    "\n",
    "        # Check conditions to determine next step\n",
    "        if state.get(\"pending_questions\"):\n",
    "            return {\"next_action\": \"clarify_questions\"}\n",
    "        elif state.get(\"test_report\") and not state.get(\"report_processed\", False):\n",
    "            return {\"next_action\": \"process_report\"}\n",
    "        elif state.get(\"symptoms_collected\", False) and state.get(\"report_processed\", False):\n",
    "            return {\"next_action\": \"follow_up\"}\n",
    "\n",
    "        return {\"next_action\": \"collect_symptoms\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[Supervisor] Error: {str(e)}\")\n",
    "        return {\"next_action\": \"await_input\"}\n",
    "\n",
    "def handle_symptoms(state: AgentState):\n",
    "    \"\"\" Collects symptoms from the patient \"\"\"\n",
    "    if not state.get(\"user_input\", \"\").strip():\n",
    "        return {\"next_action\": \"await_input\"}  # NEW state\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Ask patient about symptoms in detail.\"),\n",
    "        HumanMessage(content=state[\"user_input\"])\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages).content\n",
    "    return {\n",
    "        \"conversation_history\": state[\"conversation_history\"] + [f\"Assistant: {response}\"],\n",
    "        \"symptoms_collected\": True,\n",
    "        \"next_action\": \"supervisor\"\n",
    "    }\n",
    "\n",
    "def process_test_report(state: AgentState):\n",
    "    \"\"\" Process test report and generate questions \"\"\"\n",
    "    try:\n",
    "        if not state[\"test_report\"]:\n",
    "            return {\"next_action\": \"await_input\"}  # Wait for user to upload report\n",
    "\n",
    "        loader = Docx2txtLoader(state[\"test_report\"])\n",
    "        docs = loader.load()\n",
    "        report_content = docs[0].page_content\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"Analyze test report and generate medical questions.\"),\n",
    "            HumanMessage(content=report_content)\n",
    "        ]\n",
    "\n",
    "        questions = llm.invoke(messages).content.split(\"\\n\")\n",
    "        return {\n",
    "            \"generated_questions\": questions,\n",
    "            \"pending_questions\": questions,\n",
    "            \"report_processed\": True,\n",
    "            \"next_action\": \"supervisor\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Report Processing: {str(e)}\")\n",
    "        return {\"next_action\": \"supervisor\"}\n",
    "\n",
    "def clarify_questions(state: AgentState):\n",
    "    \"\"\" Ask the patient about test report findings \"\"\"\n",
    "    if not state[\"pending_questions\"]:\n",
    "        return {\"next_action\": \"supervisor\"}\n",
    "\n",
    "    current_question = state[\"pending_questions\"].pop(0)\n",
    "    return {\n",
    "        \"conversation_history\": state[\"conversation_history\"] + [f\"Assistant: {current_question}\"],\n",
    "        \"next_action\": \"await_input\"\n",
    "    }\n",
    "\n",
    "def follow_up(state: AgentState):\n",
    "    \"\"\" Follow-up on patient's progress \"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"Generate follow-up questions based on history.\"),\n",
    "        HumanMessage(content=\"\\n\".join(state[\"conversation_history\"]))\n",
    "    ]\n",
    "\n",
    "    follow_up_question = llm.invoke(messages).content\n",
    "    return {\n",
    "        \"conversation_history\": state[\"conversation_history\"] + [f\"Assistant: {follow_up_question}\"],\n",
    "        \"last_follow_up\": datetime.datetime.now(),\n",
    "        \"next_action\": \"await_input\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the new workflow graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"collect_symptoms\", handle_symptoms)\n",
    "workflow.add_node(\"process_report\", process_test_report)\n",
    "workflow.add_node(\"clarify_questions\", clarify_questions)\n",
    "workflow.add_node(\"follow_up\", follow_up)\n",
    "\n",
    "# NEW: `await_input` state to wait for patient input before proceeding\n",
    "workflow.add_node(\"await_input\", lambda state: {\"next_action\": \"supervisor\"})\n",
    "\n",
    "# Supervisor decides next step\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\"next_action\"],\n",
    "    {\n",
    "        \"collect_symptoms\": \"collect_symptoms\",\n",
    "        \"process_report\": \"process_report\",\n",
    "        \"clarify_questions\": \"clarify_questions\",\n",
    "        \"follow_up\": \"follow_up\",\n",
    "        \"await_input\": \"await_input\",\n",
    "        \"exit\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Each step returns to supervisor after completion\n",
    "for node in [\"collect_symptoms\", \"process_report\", \"clarify_questions\", \"follow_up\"]:\n",
    "    workflow.add_edge(node, \"supervisor\")\n",
    "\n",
    "workflow.set_entry_point(\"await_input\")  # Start by waiting for user input\n",
    "agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat interface\n",
    "def chat_interface():\n",
    "    \"\"\" Main chatbot loop \"\"\"\n",
    "    state = {\n",
    "        \"conversation_history\": [\"Assistant: Hello! I'm your health assistant. Let's start with your symptoms.\"],\n",
    "        \"test_report\": \"\",\n",
    "        \"generated_questions\": [],\n",
    "        \"pending_questions\": [],\n",
    "        \"last_follow_up\": None,\n",
    "        \"symptoms_collected\": False,\n",
    "        \"report_processed\": False,\n",
    "        \"next_action\": \"await_input\",\n",
    "        \"user_input\": \"\"\n",
    "    }\n",
    "\n",
    "    print(state[\"conversation_history\"][0])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if state[\"next_action\"] == \"await_input\":\n",
    "                user_input = input(\"\\nPatient: \")\n",
    "                state[\"user_input\"] = user_input.strip()\n",
    "            elif state[\"next_action\"] == \"process_report\":\n",
    "                report_path = input(\"\\n[System] Upload test report path: \")\n",
    "                state[\"test_report\"] = report_path.strip()\n",
    "                state[\"user_input\"] = \"[REPORT_UPLOADED]\"\n",
    "\n",
    "            result = agent.invoke(state)\n",
    "            state.update(result)\n",
    "\n",
    "            if state[\"conversation_history\"]:\n",
    "                print(f\"\\nAssistant: {state['conversation_history'][-1]}\")\n",
    "\n",
    "            if state.get(\"next_action\") == \"exit\":\n",
    "                print(\"\\n[Doctor's Summary]:\", generate_summary(state))\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            state[\"next_action\"] = \"await_input\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
